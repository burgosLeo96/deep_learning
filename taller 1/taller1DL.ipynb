{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# In this notebook, we present the results obtained of the first Deep learning exercise. The notebook is divide in two phases, the first one explains the process made for the first points that was related to predict the output of a wind turbine using a deep neural network, the second one explains the creation of a recursive neural network to predict weather features like precipitation and temperature based on historical data collected from \"Meteonet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First case: Wind Turbine Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wind Turbine "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Second case: Weather features prediction\n",
    "\n",
    "For the second case, data from NW stations was used to make the predictions. Also two univariated models where built to predict temperature and precipitation respectively. First of all, we are going to talk about data preprocessing, same process was made for both models as shown bellow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider two cases.  \n",
    "First we develop one model to predict the temperature, and other model to predict precipitation.\n",
    "We are going to show first the related functions and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from random import random\n",
    "import tensorflow as tf\n",
    "import csv\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.models import Sequential\n",
    "from hyperas import optim\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this excersise we use hyperas to help us evaluate the hyperparameters optimization\n",
    "For both cases we use the same normalize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE DATASET \n",
    "def normalize(x, stats):\n",
    "    return (x - stats['mean']) / stats['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select a window of 5000, in this case for finding X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(dataset, variable, window = 5000, future = 0):\n",
    "        data = []\n",
    "        labels = []\n",
    "        for i in range(len(dataset)):\n",
    "            start_index = i\n",
    "            end_index = i + window\n",
    "            future_index = i + window + future\n",
    "            if future_index >= len(dataset):\n",
    "                break\n",
    "            data.append(dataset[variable][i:end_index])\n",
    "            labels.append(dataset[variable][end_index:future_index])\n",
    "        return np.array(data), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperas optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function Data es reading the csv files, normalize the data and segment the data into test, train and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    TIMESTEP = '720T'\n",
    "    HISTORY_LAG = 100\n",
    "    FUTURE_TARGET = 50\n",
    "    nw_16_url = './data/NW2016.csv'\n",
    "    nw_17_url = './data/NW2017.csv'\n",
    "    nw_18_url = './data/NW2018.csv'\n",
    "\n",
    "    NW2016_dataset = pd.read_csv(nw_16_url, header = 0, sep = ',', quotechar= '\"', error_bad_lines = False)\n",
    "    NW2017_dataset = pd.read_csv(nw_17_url, header = 0, sep = ',', quotechar= '\"', error_bad_lines = False)\n",
    "    NW2018_dataset = pd.read_csv(nw_18_url, header = 0, sep = ',', quotechar= '\"', error_bad_lines = False)\n",
    "    \n",
    "    data = pd.concat([NW2016_dataset, NW2017_dataset, NW2018_dataset])\n",
    "    data = NW2016_dataset\n",
    "    data = data[data.isna()['psl'] == False]\n",
    "    #Drop useless columns\n",
    "    data = data.drop(columns = ['lat', 'lon', 'height_sta'], axis = 1)\n",
    "\n",
    "    data['date'] = pd.to_datetime(data['date'], format='%Y%m%d %H:%M')\n",
    "    data.set_index('date', inplace=True)\n",
    "    \n",
    "    #Interpolate missing values\n",
    "    data = data.interpolate(method='linear')\n",
    "\n",
    "    stats = data.describe()\n",
    "    stats = stats.transpose()\n",
    "    data = normalize(data, stats)\n",
    "    \n",
    "    resample_ds = data.resample(TIMESTEP).mean()\n",
    "\n",
    "    train_ds = resample_ds.sample(frac=0.7)\n",
    "    test_ds = resample_ds.drop(train_ds.index)\n",
    "\n",
    "    X_train, y_train = segment(train_ds, \"td\", window = HISTORY_LAG, future = FUTURE_TARGET)\n",
    "    X_train = X_train.reshape(X_train.shape[0], HISTORY_LAG, 1)\n",
    "    y_train = y_train.reshape(y_train.shape[0], FUTURE_TARGET, 1)\n",
    "\n",
    "    X_test, y_test = segment(train_ds, \"td\", window = HISTORY_LAG, future = FUTURE_TARGET)\n",
    "    X_test = X_test.reshape(X_test.shape[0], HISTORY_LAG, 1)\n",
    "    y_test = y_test.reshape(y_test.shape[0], FUTURE_TARGET,)\n",
    "\n",
    "    print(len(X_train), 'train sequences')\n",
    "    print(len(X_test), 'test sequences')\n",
    "\n",
    "    \n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print('X_test shape:', X_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the precipitation prediction this is the change that is made in this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    X_train, y_train = segment(train_ds, \"precip\", window = HISTORY_LAG, future = FUTURE_TARGET)\n",
    "    X_train = X_train.reshape(X_train.shape[0], HISTORY_LAG, 1)\n",
    "    y_train = y_train.reshape(y_train.shape[0], FUTURE_TARGET, 1)\n",
    "\n",
    "    X_test, y_test = segment(train_ds, \"precip\", window = HISTORY_LAG, future = FUTURE_TARGET)\n",
    "    X_test = X_test.reshape(X_test.shape[0], HISTORY_LAG, 1)\n",
    "    y_test = y_test.reshape(y_test.shape[0], FUTURE_TARGET,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As showed above, data was merged in just one big dataset where all the NaN values are covered with a linear interpolation function. Then, a normalization process is made with the substraction of the average value to each observation andi dividing it in the standard deviation value. Also date column is processed with a function to be able to manage it in the future.\n",
    "\n",
    "Another relevant function defined for data processing is \"segment\", that allow us to create training and labels arrays with an specific window to predict \"n\" elements in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a key step in the code showed above and is the \"resample\" function to combine the results of the entries gave an specific timestep. For this exercise, a timestep of 12 hours was defined to predict the future 50 entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construction step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are able to create the neural network model with keras, both models for precipitation and temperature prediction are shown bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model the NN\n",
    "def model(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    HISTORY_LAG = 100\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(LSTM(HISTORY_LAG, input_shape=X_train.shape[-2:]))\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('relu'))\n",
    "    model.compile(optimizer='adam',\n",
    "                       metrics=['mae', 'mse'], loss='mse')\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20)\n",
    "    checkpointer = ModelCheckpoint(filepath='keras_weights.hdf5',\n",
    "                                   verbose=1,\n",
    "                                   save_best_only=True)\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size={{choice([32, 64, 128])}},\n",
    "              epochs={{choice([100, 200, 500, 1000])}},\n",
    "              validation_split=0.08,\n",
    "              callbacks=[early_stopping, checkpointer])\n",
    "    loss, mae, mse  = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "    print('Test mae:', mae)\n",
    "    print('Test mse:', mse)\n",
    "    return {'loss': loss, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the parameters that are able to be defined in the model initialization are optimized using \"Hyperas\", an opensource library that runs the defined model multiple times and make some analyzis to choose the best combination of hyperparameters like training epochs, bratch_size, number of neurons per layer, among other features.\n",
    "\n",
    "Finally, a main method is defined to orchestrate the execution of the methods defined before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    best_run, best_model = optim.minimize(model=model,\n",
    "                                          data=data,\n",
    "                                          algo=tpe.suggest,\n",
    "                                          max_evals=10,\n",
    "                                          trials=Trials())\n",
    "    print(best_run)\n",
    "    print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"optimize\" method is the one in charge to choose the best comination of hyperparameters to improve model predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
